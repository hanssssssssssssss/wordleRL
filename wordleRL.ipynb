{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad11563",
   "metadata": {},
   "source": [
    "# Deep Q-Learning for Wordle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e0617",
   "metadata": {},
   "source": [
    "#### Content\n",
    "1. The rules of the game\n",
    "2. The concept of Deep-Q-Learning\n",
    "3. Constructing the environment\n",
    "4. The architecture of the model\n",
    "5. Training results and tweaks\n",
    "6. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aff0c5",
   "metadata": {},
   "source": [
    "# 1. The rules of wordle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252be6be",
   "metadata": {},
   "source": [
    "- Guess a five letter word in up to six attempts.\n",
    "- After eacht attempt: Feedback for letters that are at the correct position / somewhere else / not at all in the solution.\n",
    "- In the origibnal version: 12972 allowed guesswords, 2309 potential solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d33fa",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deced0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordle import Wordle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b963c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = \"bread\"\n",
    "game = Wordle(vocab=None, solution=solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6680589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=background-color:yellow;font-size:large>a</text><text style=background-color:lightgrey;font-size:large>p</text><text style=background-color:lightgrey;font-size:large>p</text><text style=background-color:lightgrey;font-size:large>l</text><text style=background-color:yellow;font-size:large>e</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=background-color:lightgreen;font-size:large>b</text><text style=background-color:yellow;font-size:large>e</text><text style=background-color:yellow;font-size:large>r</text><text style=background-color:lightgrey;font-size:large>r</text><text style=background-color:lightgrey;font-size:large>y</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=background-color:lightgreen;font-size:large>b</text><text style=background-color:lightgreen;font-size:large>r</text><text style=background-color:yellow;font-size:large>a</text><text style=background-color:lightgrey;font-size:large>k</text><text style=background-color:yellow;font-size:large>e</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style=background-color:lightgreen;font-size:large>b</text><text style=background-color:lightgreen;font-size:large>r</text><text style=background-color:lightgreen;font-size:large>e</text><text style=background-color:lightgreen;font-size:large>a</text><text style=background-color:lightgreen;font-size:large>d</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over, you  won!\n"
     ]
    }
   ],
   "source": [
    "game.play_visual(\"apple\")\n",
    "game.play_visual(\"berry\")\n",
    "game.play_visual(\"brake\")\n",
    "game.play_visual(\"bread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d230a7",
   "metadata": {},
   "source": [
    "**Disclaimer:** This can be solved analytically with information theory. At each step there is an optimal guessword choice that minimizes entropy. But we're not here to do any of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc9c9e",
   "metadata": {},
   "source": [
    "After each guess, the game responds with one of $3^5$ seqeunces of green/yellow/grey-tiles. This means the potential number of different states per game is $(12972 * 3^5)^6 = 9.81 * 10^{38}$.  \n",
    "This is way too big to be explicitly represented in e.g. a Q-table.  \n",
    "&rarr; The expected reward has to be approximated by a parameterized function (a neural network) as a representation of policy π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97caf8af",
   "metadata": {},
   "source": [
    "# 2. Deep-Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9dad2b",
   "metadata": {},
   "source": [
    "The idea behind Deep Q-Learning is to expand the single-step online updating of regular Q-learning with *experience replay* where the agent stores previous experience in a replay memory and periodically draws random samples from this memory to use as training data batches\n",
    "- Experience can potentially be used in many weight updates &rarr; greater data efficiency\n",
    "- Correlations between consecutive samples are broken &rarr; reduce the variance of updates\n",
    "- Experiences from other action-sequences are used &rarr; less susceptible to path dependencies in online training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866c9e8",
   "metadata": {},
   "source": [
    "    Initialize action-value function Q with random weights\n",
    "    for episodes M do\n",
    "        for time t do  \n",
    "            With probability ε:                \n",
    "                select action aₜ randomly\n",
    "            else \n",
    "                select aₜ = max Q(φ(sₜ), θ)  \n",
    "            Execute action aₜ \n",
    "            Observe reward rₜ and state sₜ₊₁ \n",
    "            Store transition (sₜ, aₜ, rₜ, sₜ₊₁) in D  \n",
    "            Sample random minibatch (sₖ, aₖ, rₖ, sₖ₊₁) from replay memory\n",
    "            \n",
    "            if episode terminates in sₖ₊₁\n",
    "                yₖ = rₖ\n",
    "            else\n",
    "                Set yₖ = rₖ + γ  * max Q(φ(sₖ₊₁), θ) \n",
    "            Perform gradient descent step on (yₖ − Q(φₖ, θ))²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb43eb4",
   "metadata": {},
   "source": [
    "# 3. Constructing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abae1d",
   "metadata": {},
   "source": [
    "### State space: \n",
    "- After each guess the game returns information for each letter at each position of the guessed word.\n",
    "- In the visual representation this is one of $3^5$ combinations of green/yellow/grey tiles.\n",
    "- The information of previus guesses should be included in the current state. This can be done e.g with a state vector of length $417$ that stores the following information:\n",
    "     - `state[0]` for the index of the current round.\n",
    "     - `state[1:26]` to remeber which letters have been guessed.\n",
    "     - After that: $5 * 26 * 3$ features for the green/yellow/grey-information of each letter in each position.\n",
    "         + Letter is definitely not in this spot: `[1, 0, 0]` \n",
    "         + Letter is maybe in this spot: `[0, 1, 0]`\n",
    "         + Letter is definitely in this spot: `[0, 0, 1]`\n",
    "- Reward can be given for correct letters and won games, negative reward for lost games.\n",
    "\n",
    "### Action space:\n",
    "- Originally: In each round any choice from the ~13k vocabulary is allowed. But only ~2.3k of these are poteniall solutions.\n",
    "- To reduce the size of the action space, this environment limits the vocabulary to words that are potenital solutions.  \n",
    "&rarr; The action space can be represented as a vector of vocabulary-length and interpreted eg. as a probability distribution for each word at the next round.\n",
    "\n",
    "### Reward:\n",
    "Balanced reward/cost for winnig and loosing:\n",
    "+ $- 50$ for a lost game\n",
    "+ $+ 50$ for a win"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d3796c",
   "metadata": {},
   "source": [
    "# 4. The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9b98a",
   "metadata": {},
   "source": [
    "### General architecture\n",
    "- The used model is constructed as a linear network with one hidden layer of size 256 and stochastic gradient descent optimization.  \n",
    "- The model's input layer is of size $417$ to fit the state vector of the wordle environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddc7d4",
   "metadata": {},
   "source": [
    "### The model output\n",
    "Each word from the vocabulary can be represented as a one hot encoded set of characters in each position, resulting in a vector of length $5 * 26 = 130$. This means the vocabulary itself can be represented as a matrix with 130 columns.  \n",
    "This allows for a model with one of the following two ways to predict the next action (= choose a word from the vocabulary)\n",
    "1. Use an output layer with a node for each word in the vocabulary\n",
    "    - Simplest way to obtain action probabilities\n",
    "    - The defining feature of a word is it's index position in the vocaulary\n",
    "2. Use an output layer of length 130 that is multiplied with the vocabulary matrix in the forwarding step\n",
    "    - The output layer can be semantically interpreted as weights for each letter and position\n",
    "    - Similar words get similarly activated\n",
    "    - The vocabulary can be shuffeled/changed  \n",
    "    &rarr; Allows warm-starting of agents for larger vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a4714",
   "metadata": {},
   "source": [
    "# 5. Training results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7a8dc",
   "metadata": {},
   "source": [
    "Training was started with a minimal variant of the problem setting for bugfixing and general proof of concept. Training on a 100 word vocabulary with 20 possible solutions shows good results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b14a991",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_winrate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3216994/1928134596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mn_games\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrecent_wins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtotal_winrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecent_wins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_winrate' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_games = []\n",
    "recent_wins = []\n",
    "total_winrate = []\n",
    "\n",
    "with open(\"stats/v100s20.txt\") as stats_small:\n",
    "    rows = csv.reader(stats_small, delimiter=\",\")\n",
    "    for row in rows:\n",
    "        n_games.append(int(row[0]))\n",
    "        recent_wins.append(int(row[1]))\n",
    "        total_winrate.append(float(row[2]))\n",
    "        \n",
    "plt.scatter(x=n_games, y=recent_wins)\n",
    "plt.xlabel(\"Number of games\")\n",
    "plt.ylabel(\"Wins per 100\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6ffe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8404da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "vocab_subset_len = 100\n",
    "random_seed = None\n",
    "with open(\"data/possible_words.txt\") as word_list:\n",
    "    vocab = word_list.read().split(\"\\n\")\n",
    "if vocab_subset_len:\n",
    "    random.seed(random_seed)\n",
    "    vocab = random.sample(vocab, k=vocab_subset_len)\n",
    "random.seed(random_seed)\n",
    "solution = random.choice(vocab)\n",
    "game = Wordle(vocab, 6, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2705d",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db09697",
   "metadata": {},
   "source": [
    "- Anderson, Benton et al. (2022): [Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning](https://arxiv.org/abs/2202.00557)\n",
    "- Ho, Andrew (2022): [Solving Wordle with Reinforcement Learning](https://wandb.ai/andrewkho/wordle-solver/reports/Solving-Wordle-with-Reinforcement-Learning--VmlldzoxNTUzOTc4)\n",
    "- Loeber, Patrick (2022): [Train an AI to Play Snake](https://www.youtube.com/watch?v=L8ypSXwyBds) \n",
    "- Mnih, Volodymyr (2018): [Deep RL Bootcamp Lecture Deep Q-Networks](https://www.youtube.com/watch?v=fevMOp5TDQs)\n",
    "- Mnih, Volodymyr et al.(2013): [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n",
    "- Sanderson, Grant (2022): [Solving Wordle using information theory](https://www.youtube.com/watch?v=v68zYyaEmEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d79c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
